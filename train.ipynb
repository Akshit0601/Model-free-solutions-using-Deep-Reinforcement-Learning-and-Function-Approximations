{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf6b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102278/2865488907.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from random import random\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d20015",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1.8,-0.77],[1,0]])\n",
    "B = np.array([[1],\n",
    "              [0]])\n",
    "C = np.array([1,0.8])\n",
    "gama = 0.97\n",
    "alpha_c = 0.1\n",
    "lam = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0580f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.autograd.functional import jacobian\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def model(i,o):\n",
    "    model = nn.Sequential(\n",
    "    nn.Linear(i, 10),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(10, o)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def DNN(func,optimizer, n,x_sample,y_sample,xk, require_jack = False):\n",
    "    x_sample = np.array(x_sample)\n",
    "    # func = model(x_sample.shape[1],o)\n",
    "    # optimizer = torch.optim.Adam(func.parameters(), lr=0.01)\n",
    "    # loss_fn = nn.MSELoss()\n",
    "    dists = np.linalg.norm(x_sample-xk,axis=1)\n",
    "    K = min(n,len(dists))\n",
    "    nearest_idx = np.argsort(dists)[:K]\n",
    "    x_mask = x_sample[nearest_idx]\n",
    "    y_mask = np.array(y_sample)[nearest_idx]\n",
    "    X = torch.tensor(x_mask).float()\n",
    "    Y = torch.tensor(y_mask).float()\n",
    "    for epoch in range(500):\n",
    "        pred = func(X)\n",
    "        loss = loss_fn(pred, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    grad = return_gradient(func,xk,require_jac=require_jack)\n",
    "    return grad, nearest_idx\n",
    "\n",
    "def return_gradient(model, xk, require_jac = False):\n",
    "   \n",
    "    x_test = torch.tensor(xk).float().requires_grad_(True)\n",
    "    y_target = model(x_test)\n",
    "    x_test.grad = None\n",
    "    model.zero_grad()\n",
    "    gradient = torch.zeros(2)\n",
    "    if not require_jac:\n",
    "        y_target.backward()\n",
    "        return x_test.grad\n",
    "    else:\n",
    "        for i in range(2):\n",
    "            x_test.grad = None\n",
    "            model.zero_grad()\n",
    "            y_target[i].backward(retain_graph = True)\n",
    "            gradient[i] = x_test.grad[2]\n",
    "        return gradient\n",
    "            \n",
    "def saturate(u, limit=5.0):\n",
    "    return np.clip(u, -limit, limit)\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd4deed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshit/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/akshit/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_c:  tensor([ 0.2084, -0.1166]) grad_a:  tensor([-0.0543,  0.1576]) grad_p: tensor([0.2811, 0.0641])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshit/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_c:  tensor([ 0.1976, -0.0949]) grad_a:  tensor([-0.0722,  0.1159]) grad_p: tensor([-0.2917, -0.1990])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshit/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_c:  tensor([0.0915, 0.0507]) grad_a:  tensor([-0.0695,  0.0535]) grad_p: tensor([-0.6817, -0.6164])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshit/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_c:  tensor([0.2151, 0.0409]) grad_a:  tensor([-0.0542,  0.0437]) grad_p: tensor([-1.7924, -1.5469])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshit/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_c:  tensor([0.0868, 0.3557]) grad_a:  tensor([-0.0683,  0.0808]) grad_p: tensor([-3.1786, -2.7955])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshit/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_c:  tensor([-0.0559,  0.5819]) grad_a:  tensor([-0.0591,  0.0724]) grad_p: tensor([-3.9399, -3.5356])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshit/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_c:  tensor([0.2884, 0.1747]) grad_a:  tensor([0.0446, 0.1094]) grad_p: tensor([-1.2951e-05, -1.3511e-05])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshit/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_c:  tensor([0.2287, 0.0776]) grad_a:  tensor([-0.0265,  0.1006]) grad_p: tensor([-1.0171, -0.6808])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshit/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_c:  tensor([0.1956, 0.1141]) grad_a:  tensor([-0.0501,  0.0735]) grad_p: tensor([-1.1680, -0.7492])\n",
      "grad_c:  tensor([0.1335, 0.4015]) grad_a:  tensor([-0.0333,  0.0440]) grad_p: tensor([-1.2975, -1.3261])\n",
      "grad_c:  tensor([-0.1368,  1.0038]) grad_a:  tensor([-0.0210,  0.0248]) grad_p: tensor([-0.6112, -0.9081])\n",
      "grad_c:  tensor([-1.0051,  2.5338]) grad_a:  tensor([-0.0073,  0.0082]) grad_p: tensor([-1.3460, -1.4872])\n",
      "grad_c:  tensor([-1.5440,  3.1094]) grad_a:  tensor([-0.0023,  0.0026]) grad_p: tensor([6.0324, 4.3882])\n",
      "grad_c:  tensor([ 0.1201, -0.1465]) grad_a:  tensor([0.2152, 0.2160]) grad_p: tensor([13.6368, 10.3483])\n",
      "grad_c:  tensor([ 0.3465, -0.4510]) grad_a:  tensor([ 0.0019, -0.0167]) grad_p: tensor([0.2904, 0.2889])\n",
      "grad_c:  tensor([ 0.5564, -0.3527]) grad_a:  tensor([-0.0094,  0.0183]) grad_p: tensor([0.8052, 0.4921])\n",
      "grad_c:  tensor([-0.1188,  0.4801]) grad_a:  tensor([ 0.0025, -0.0018]) grad_p: tensor([0.7630, 0.2541])\n",
      "grad_c:  tensor([-0.9342,  1.9544]) grad_a:  tensor([ 0.0021, -0.0026]) grad_p: tensor([0.6391, 0.0541])\n",
      "grad_c:  tensor([-2.6026,  4.0833]) grad_a:  tensor([-0.0006,  0.0007]) grad_p: tensor([-0.5071, -0.8095])\n",
      "grad_c:  tensor([-5.2117,  8.0670]) grad_a:  tensor([ 7.0826e-05, -8.2644e-05]) grad_p: tensor([-0.8625, -1.3289])\n",
      "grad_c:  tensor([-0.8678,  1.6705]) grad_a:  tensor([-0.0032, -0.0011]) grad_p: tensor([56.4741, 44.7069])\n",
      "grad_c:  tensor([ 0.3833, -0.2897]) grad_a:  tensor([-0.0042, -0.0036]) grad_p: tensor([0.5000, 0.0879])\n",
      "grad_c:  tensor([0.0704, 0.1402]) grad_a:  tensor([0.0002, 0.0005]) grad_p: tensor([0.9077, 0.1952])\n",
      "grad_c:  tensor([-0.0777,  0.7996]) grad_a:  tensor([-0.0005,  0.0003]) grad_p: tensor([0.8848, 0.0805])\n",
      "grad_c:  tensor([-0.2031,  0.9155]) grad_a:  tensor([ 0.0027, -0.0030]) grad_p: tensor([ 0.6676, -0.1389])\n",
      "grad_c:  tensor([-3.6317,  6.3782]) grad_a:  tensor([ 0.0004, -0.0004]) grad_p: tensor([-0.5289, -1.1402])\n",
      "grad_c:  tensor([-1.0380,  1.9135]) grad_a:  tensor([ 0.0013, -0.0015]) grad_p: tensor([64.9262, 50.5095])\n",
      "grad_c:  tensor([-2.7694,  3.2160]) grad_a:  tensor([-0.0059, -0.0009]) grad_p: tensor([-6.1032, -5.0103])\n",
      "grad_c:  tensor([-0.1137,  0.1397]) grad_a:  tensor([0.0312, 0.0347]) grad_p: tensor([ 0.0877, -0.1136])\n",
      "grad_c:  tensor([-4.5772,  1.2316]) grad_a:  tensor([-0.0038, -0.0019]) grad_p: tensor([1.1830, 0.6206])\n",
      "grad_c:  tensor([ 0.4104, -0.0546]) grad_a:  tensor([ 4.9141e-05, -3.3442e-04]) grad_p: tensor([ 1.1262, -0.1198])\n",
      "grad_c:  tensor([0.5328, 0.1266]) grad_a:  tensor([ 0.0005, -0.0005]) grad_p: tensor([ 0.7366, -0.2210])\n",
      "grad_c:  tensor([0.8358, 0.1476]) grad_a:  tensor([ 5.8832e-05, -4.6011e-05]) grad_p: tensor([-0.0258, -0.9512])\n",
      "grad_c:  tensor([-1.9465,  4.4571]) grad_a:  tensor([-1.5523e-04,  2.1243e-05]) grad_p: tensor([-0.6182, -1.9630])\n",
      "grad_c:  tensor([-2.2530,  2.3541]) grad_a:  tensor([-0.0015, -0.0030]) grad_p: tensor([-4.9712, -3.5484])\n",
      "grad_c:  tensor([ 0.0348, -1.2809]) grad_a:  tensor([-0.0016, -0.0022]) grad_p: tensor([-0.5264, -0.4224])\n",
      "grad_c:  tensor([ 0.7579, -2.5126]) grad_a:  tensor([-5.8122e-05,  1.4626e-04]) grad_p: tensor([-0.1645, -0.2164])\n",
      "grad_c:  tensor([ 2.0401, -2.8153]) grad_a:  tensor([-0.0008,  0.0014]) grad_p: tensor([ 0.2938, -0.2268])\n",
      "grad_c:  tensor([ 1.1656, -0.2916]) grad_a:  tensor([-0.0003,  0.0008]) grad_p: tensor([-0.4682, -0.8606])\n",
      "grad_c:  tensor([ 1.8118, -0.1640]) grad_a:  tensor([ 0.0015, -0.0017]) grad_p: tensor([-1.7956, -1.5659])\n",
      "grad_c:  tensor([0.9895, 0.4438]) grad_a:  tensor([ 0.0007, -0.0007]) grad_p: tensor([-5.4921, -4.7997])\n",
      "grad_c:  tensor([ 0.0043, -1.9458]) grad_a:  tensor([-0.0014, -0.0003]) grad_p: tensor([-6.2809, -5.0191])\n",
      "grad_c:  tensor([ 1.5217, -4.8073]) grad_a:  tensor([-0.0014,  0.0015]) grad_p: tensor([-0.0210, -0.0273])\n",
      "grad_c:  tensor([ 2.3393, -5.6037]) grad_a:  tensor([ 0.0019, -0.0015]) grad_p: tensor([0.0067, 0.0030])\n",
      "grad_c:  tensor([ 2.9124, -2.0132]) grad_a:  tensor([ 0.0011, -0.0005]) grad_p: tensor([-0.4258, -0.3505])\n",
      "grad_c:  tensor([ 4.1567, -2.2731]) grad_a:  tensor([ 0.0002, -0.0001]) grad_p: tensor([-1.3309, -1.1059])\n",
      "grad_c:  tensor([ 5.5809, -4.9101]) grad_a:  tensor([ 1.2882e-04, -3.6410e-05]) grad_p: tensor([1.4643, 0.8620])\n",
      "grad_c:  tensor([-0.0599, -0.8941]) grad_a:  tensor([-0.0035,  0.0004]) grad_p: tensor([-0.0565, -0.0452])\n",
      "grad_c:  tensor([ 0.5606, -3.0433]) grad_a:  tensor([-6.9236e-05, -1.2175e-03]) grad_p: tensor([0.0966, 0.0455])\n",
      "grad_c:  tensor([ 0.6392, -5.7741]) grad_a:  tensor([ 0.0016, -0.0005]) grad_p: tensor([0.1209, 0.0681])\n",
      "grad_c:  tensor([ 2.3436, -1.3971]) grad_a:  tensor([ 0.0004, -0.0004]) grad_p: tensor([-0.2011, -0.1536])\n",
      "grad_c:  tensor([ 6.1225, -4.2091]) grad_a:  tensor([ 8.1644e-05, -1.6663e-04]) grad_p: tensor([-0.8350, -0.6632])\n",
      "grad_c:  tensor([ 7.7816, -6.9074]) grad_a:  tensor([ 1.3188e-05, -3.1402e-05]) grad_p: tensor([3.4750, 2.4040])\n",
      "grad_c:  tensor([-0.6812,  0.2057]) grad_a:  tensor([-0.0008,  0.0012]) grad_p: tensor([-1.1015, -0.8349])\n",
      "grad_c:  tensor([ 0.9567, -3.7286]) grad_a:  tensor([-0.0007,  0.0013]) grad_p: tensor([0.0817, 0.0558])\n",
      "grad_c:  tensor([-2.0120, -2.4344]) grad_a:  tensor([-0.0017,  0.0035]) grad_p: tensor([-0.1874, -0.1114])\n",
      "grad_c:  tensor([ 4.2012, -3.8291]) grad_a:  tensor([ 0.0010, -0.0005]) grad_p: tensor([-0.6472, -0.4525])\n",
      "grad_c:  tensor([ 7.7498, -5.3860]) grad_a:  tensor([ 0.0017, -0.0016]) grad_p: tensor([-1.8279, -1.4191])\n",
      "grad_c:  tensor([-0.2933, -0.6877]) grad_a:  tensor([-0.0041,  0.0030]) grad_p: tensor([-5.8401, -4.7351])\n",
      "grad_c:  tensor([ 0.7302, -4.6570]) grad_a:  tensor([-0.0045,  0.0065]) grad_p: tensor([0.5485, 0.2913])\n",
      "grad_c:  tensor([-3.0974, -2.5733]) grad_a:  tensor([-7.8271e-05, -1.8889e-03]) grad_p: tensor([-0.0190, -0.0155])\n",
      "grad_c:  tensor([-0.5738,  0.7071]) grad_a:  tensor([ 0.0021, -0.0016]) grad_p: tensor([-0.6292, -0.4348])\n",
      "grad_c:  tensor([ 4.3067, -1.5502]) grad_a:  tensor([ 0.0010, -0.0006]) grad_p: tensor([-1.5912, -1.3004])\n",
      "grad_c:  tensor([ 9.7049, -8.2954]) grad_a:  tensor([ 0.0016, -0.0019]) grad_p: tensor([-17.1422, -13.5967])\n",
      "grad_c:  tensor([-1.2236, -0.1437]) grad_a:  tensor([-0.0002, -0.0004]) grad_p: tensor([-0.0232, -0.0177])\n",
      "grad_c:  tensor([-6.4410,  6.1111]) grad_a:  tensor([-1.5621e-04, -6.5050e-05]) grad_p: tensor([0.2802, 0.1630])\n",
      "grad_c:  tensor([-0.0111, -3.0784]) grad_a:  tensor([ 8.1338e-05, -1.2882e-04]) grad_p: tensor([0.7102, 0.5502])\n",
      "grad_c:  tensor([-1.4653, -2.1659]) grad_a:  tensor([ 0.0009, -0.0018]) grad_p: tensor([0.2223, 0.1777])\n",
      "grad_c:  tensor([ 4.8542, -3.0647]) grad_a:  tensor([-8.9604e-05, -2.2850e-05]) grad_p: tensor([-0.7619, -0.6310])\n",
      "grad_c:  tensor([ 8.2273, -4.9159]) grad_a:  tensor([ 0.0002, -0.0002]) grad_p: tensor([ 0.0801, -0.0236])\n",
      "grad_c:  tensor([-0.7829, -1.0483]) grad_a:  tensor([-0.0015,  0.0025]) grad_p: tensor([-4.8260, -3.7963])\n",
      "grad_c:  tensor([-1.1955, -1.9875]) grad_a:  tensor([ 0.0013, -0.0047]) grad_p: tensor([1.1392, 0.8090])\n",
      "grad_c:  tensor([-0.2213, -4.3564]) grad_a:  tensor([-0.0006,  0.0007]) grad_p: tensor([0.5600, 0.3490])\n",
      "grad_c:  tensor([-0.4597, -2.2059]) grad_a:  tensor([-0.0005,  0.0004]) grad_p: tensor([0.8752, 0.5719])\n",
      "grad_c:  tensor([ 7.2673, -6.1531]) grad_a:  tensor([-0.0003,  0.0003]) grad_p: tensor([1.0459, 0.8329])\n",
      "grad_c:  tensor([ 9.9963, -7.0600]) grad_a:  tensor([-0.0003,  0.0004]) grad_p: tensor([0.2093, 0.1813])\n",
      "grad_c:  tensor([-1.3535, -1.0811]) grad_a:  tensor([0.0004, 0.0001]) grad_p: tensor([-0.0079, -0.0064])\n",
      "grad_c:  tensor([-14.8633,   7.3565]) grad_a:  tensor([-0.0005, -0.0010]) grad_p: tensor([1.1633, 0.5208])\n",
      "grad_c:  tensor([ 0.4665, -5.0608]) grad_a:  tensor([-0.0006,  0.0011]) grad_p: tensor([0.9795, 0.4724])\n",
      "grad_c:  tensor([-0.2295, -2.2197]) grad_a:  tensor([-0.0044,  0.0043]) grad_p: tensor([0.7655, 0.4274])\n",
      "grad_c:  tensor([ 6.0995, -6.6076]) grad_a:  tensor([ 0.0008, -0.0009]) grad_p: tensor([0.3816, 0.2594])\n",
      "grad_c:  tensor([11.8013, -8.4722]) grad_a:  tensor([-0.0034,  0.0039]) grad_p: tensor([-0.3444, -0.2682])\n",
      "grad_c:  tensor([-5.5547,  2.4219]) grad_a:  tensor([ 0.0035, -0.0018]) grad_p: tensor([0.4183, 0.2771])\n",
      "grad_c:  tensor([-0.1945, -0.0339]) grad_a:  tensor([0.0026, 0.0043]) grad_p: tensor([0.3444, 0.2289])\n",
      "grad_c:  tensor([-0.5592, -0.5100]) grad_a:  tensor([0.0002, 0.0007]) grad_p: tensor([1.0491, 0.6033])\n",
      "grad_c:  tensor([ 0.0558, -5.6209]) grad_a:  tensor([ 0.0007, -0.0008]) grad_p: tensor([0.9709, 0.5636])\n",
      "grad_c:  tensor([-0.5367, -2.2088]) grad_a:  tensor([ 0.0004, -0.0005]) grad_p: tensor([0.6884, 0.4304])\n",
      "grad_c:  tensor([ 4.5903, -4.3849]) grad_a:  tensor([ 0.0006, -0.0007]) grad_p: tensor([0.0508, 0.0400])\n",
      "grad_c:  tensor([10.9248, -7.2657]) grad_a:  tensor([-0.0039,  0.0043]) grad_p: tensor([0.5065, 0.4436])\n",
      "grad_c:  tensor([-1.3435,  0.3710]) grad_a:  tensor([-0.0020, -0.0042]) grad_p: tensor([-0.0005, -0.0004])\n",
      "grad_c:  tensor([-0.6249, -0.3083]) grad_a:  tensor([-0.0020,  0.0029]) grad_p: tensor([-0.0044, -0.0033])\n",
      "grad_c:  tensor([-0.1261, -2.9431]) grad_a:  tensor([-0.0030,  0.0069]) grad_p: tensor([-0.0051, -0.0039])\n",
      "grad_c:  tensor([-0.6065, -5.0384]) grad_a:  tensor([-0.0009,  0.0009]) grad_p: tensor([-0.0025, -0.0020])\n",
      "grad_c:  tensor([ 0.3636, -6.2499]) grad_a:  tensor([ 0.0006, -0.0003]) grad_p: tensor([-0.0008, -0.0006])\n",
      "grad_c:  tensor([ 8.0180, -4.1349]) grad_a:  tensor([ 0.0010, -0.0003]) grad_p: tensor([-0.0003, -0.0002])\n",
      "grad_c:  tensor([ 5.4821, -3.3995]) grad_a:  tensor([-0.0009,  0.0014]) grad_p: tensor([-15.7238, -12.8586])\n",
      "grad_c:  tensor([-0.5590,  0.0637]) grad_a:  tensor([-0.0004, -0.0010]) grad_p: tensor([-0.0014, -0.0011])\n",
      "grad_c:  tensor([-15.1364,  11.8836]) grad_a:  tensor([-7.5384e-05, -9.4068e-04]) grad_p: tensor([-0.9917, -0.7489])\n",
      "grad_c:  tensor([ 0.0134, -4.2587]) grad_a:  tensor([-0.0009,  0.0005]) grad_p: tensor([-0.7347, -0.5568])\n",
      "grad_c:  tensor([-0.2901, -2.2927]) grad_a:  tensor([-7.2025e-05,  3.2382e-04]) grad_p: tensor([-0.0095, -0.0073])\n",
      "grad_c:  tensor([-0.3369, -1.8617]) grad_a:  tensor([ 1.0799e-04, -9.3762e-05]) grad_p: tensor([-1.8316, -1.4612])\n",
      "grad_c:  tensor([10.9578, -6.2258]) grad_a:  tensor([-0.0034,  0.0040]) grad_p: tensor([-1.8704, -1.4982])\n",
      "grad_c:  tensor([-0.7203,  0.2909]) grad_a:  tensor([0.0438, 0.0165]) grad_p: tensor([-5.5616, -4.0490])\n",
      "grad_c:  tensor([-1.6145,  0.5286]) grad_a:  tensor([0.0084, 0.0046]) grad_p: tensor([-0.1706, -0.1277])\n",
      "grad_c:  tensor([-0.1125, -2.3212]) grad_a:  tensor([-0.0048,  0.0068]) grad_p: tensor([-0.0396, -0.0300])\n",
      "grad_c:  tensor([-1.5769, -2.4724]) grad_a:  tensor([-0.0016,  0.0009]) grad_p: tensor([-0.0042, -0.0032])\n",
      "grad_c:  tensor([-0.7976, -3.6562]) grad_a:  tensor([-0.0002,  0.0002]) grad_p: tensor([-0.0154, -0.0112])\n",
      "grad_c:  tensor([ 9.9615, -5.1984]) grad_a:  tensor([9.4652e-05, 3.9277e-05]) grad_p: tensor([-0.0007, -0.0005])\n",
      "grad_c:  tensor([-1.4510, -0.4057]) grad_a:  tensor([0.0079, 0.0108]) grad_p: tensor([-3.7654, -2.6990])\n",
      "grad_c:  tensor([-0.7632, -0.2701]) grad_a:  tensor([-0.0016, -0.0027]) grad_p: tensor([-0.0063, -0.0047])\n",
      "grad_c:  tensor([-0.4098, -3.6603]) grad_a:  tensor([ 9.2789e-05, -5.4429e-03]) grad_p: tensor([-0.0088, -0.0067])\n",
      "grad_c:  tensor([-1.9182, -3.8733]) grad_a:  tensor([-0.0051,  0.0012]) grad_p: tensor([-0.0008, -0.0006])\n",
      "grad_c:  tensor([-0.6822, -4.2443]) grad_a:  tensor([-0.0004,  0.0008]) grad_p: tensor([-0.0014, -0.0010])\n",
      "grad_c:  tensor([-3.1592,  7.9289]) grad_a:  tensor([-0.0001, -0.0004]) grad_p: tensor([-0.0013, -0.0009])\n",
      "grad_c:  tensor([-1.4345,  6.7479]) grad_a:  tensor([ 2.8971e-05, -8.5097e-05]) grad_p: tensor([-8.1165, -6.0518])\n",
      "grad_c:  tensor([-1.5325,  1.3002]) grad_a:  tensor([0.0123, 0.0090]) grad_p: tensor([-0.0178, -0.0133])\n",
      "grad_c:  tensor([-0.0525, -0.0100]) grad_a:  tensor([0.0083, 0.0097]) grad_p: tensor([-0.0022, -0.0017])\n",
      "grad_c:  tensor([ 0.0041, -0.0881]) grad_a:  tensor([ 0.0022, -0.0102]) grad_p: tensor([-0.0038, -0.0029])\n",
      "grad_c:  tensor([  1.1826, -10.5681]) grad_a:  tensor([-0.0042,  0.0014]) grad_p: tensor([-0.0002, -0.0002])\n",
      "grad_c:  tensor([-0.8303, -5.7817]) grad_a:  tensor([-0.0016,  0.0012]) grad_p: tensor([-0.0004, -0.0003])\n",
      "grad_c:  tensor([0.2396, 1.7056]) grad_a:  tensor([-0.0005, -0.0008]) grad_p: tensor([-0.0148, -0.0110])\n",
      "grad_c:  tensor([3.8926, 5.5666]) grad_a:  tensor([-5.7752e-05, -8.5806e-05]) grad_p: tensor([-3.2442e-06, -2.6503e-06])\n",
      "grad_c:  tensor([-0.2716,  0.1981]) grad_a:  tensor([0.0092, 0.0095]) grad_p: tensor([-1.5168e-05, -1.1667e-05])\n",
      "grad_c:  tensor([-3.5804, -0.5270]) grad_a:  tensor([-0.0076, -0.0072]) grad_p: tensor([-0.0028, -0.0021])\n",
      "grad_c:  tensor([-0.3112, -1.5423]) grad_a:  tensor([-0.0139, -0.0242]) grad_p: tensor([-0.0011, -0.0008])\n",
      "grad_c:  tensor([-1.3302, -3.6456]) grad_a:  tensor([-0.0034, -0.0023]) grad_p: tensor([-0.0003, -0.0002])\n",
      "grad_c:  tensor([ 2.9567, -9.2134]) grad_a:  tensor([ 0.0014, -0.0008]) grad_p: tensor([-1.0225e-04, -8.6487e-05])\n",
      "grad_c:  tensor([-2.6614,  7.9610]) grad_a:  tensor([ 1.4622e-04, -4.9249e-05]) grad_p: tensor([-1.0379e-05, -8.7521e-06])\n",
      "grad_c:  tensor([-0.0314, -0.0946]) grad_a:  tensor([0.0080, 0.0098]) grad_p: tensor([-0.0020, -0.0016])\n",
      "grad_c:  tensor([-1.0665,  0.0781]) grad_a:  tensor([-0.0013, -0.0001]) grad_p: tensor([0.2236, 0.1656])\n",
      "grad_c:  tensor([-1.0178, -4.9087]) grad_a:  tensor([ 0.0010, -0.0013]) grad_p: tensor([0.1908, 0.1308])\n",
      "grad_c:  tensor([-2.2792, -3.1482]) grad_a:  tensor([ 4.2185e-05, -9.4255e-04]) grad_p: tensor([0.1671, 0.1066])\n",
      "grad_c:  tensor([ 2.6316, -6.7670]) grad_a:  tensor([ 0.0003, -0.0002]) grad_p: tensor([-0.1668, -0.1282])\n",
      "grad_c:  tensor([3.1497, 1.8690]) grad_a:  tensor([ 0.0004, -0.0005]) grad_p: tensor([-0.3564, -0.3054])\n",
      "grad_c:  tensor([-0.1456, -0.0303]) grad_a:  tensor([0.0056, 0.0195]) grad_p: tensor([-3.6414e-06, -2.7223e-06])\n",
      "grad_c:  tensor([-1.4406, -0.2958]) grad_a:  tensor([-0.0035, -0.0018]) grad_p: tensor([0.5447, 0.4536])\n",
      "grad_c:  tensor([-1.4940, -2.4340]) grad_a:  tensor([-0.0059,  0.0029]) grad_p: tensor([0.6968, 0.5282])\n",
      "grad_c:  tensor([-1.9913, -1.7606]) grad_a:  tensor([-0.0052,  0.0016]) grad_p: tensor([0.3683, 0.2245])\n",
      "grad_c:  tensor([  4.5553, -12.8434]) grad_a:  tensor([-0.0009,  0.0014]) grad_p: tensor([0.3294, 0.2108])\n",
      "grad_c:  tensor([3.3168, 3.2277]) grad_a:  tensor([-7.3369e-06, -5.2659e-05]) grad_p: tensor([0.2714, 0.1831])\n",
      "grad_c:  tensor([-0.2723,  0.0696]) grad_a:  tensor([-0.0028,  0.0044]) grad_p: tensor([-1.9333e-06, -1.4321e-06])\n",
      "grad_c:  tensor([-16.8072,  -1.9665]) grad_a:  tensor([-0.0037,  0.0016]) grad_p: tensor([-0.0003, -0.0002])\n",
      "grad_c:  tensor([-0.9201, -2.2314]) grad_a:  tensor([-0.0243,  0.0204]) grad_p: tensor([-0.0006, -0.0004])\n",
      "grad_c:  tensor([-2.0479, -2.5370]) grad_a:  tensor([-0.0080,  0.0058]) grad_p: tensor([-0.0002, -0.0002])\n",
      "grad_c:  tensor([  4.5483, -12.1381]) grad_a:  tensor([0.0004, 0.0007]) grad_p: tensor([-2.2880e-05, -1.6456e-05])\n",
      "grad_c:  tensor([0.8451, 4.7970]) grad_a:  tensor([-0.0003,  0.0003]) grad_p: tensor([-7.1920e-07, -5.1583e-07])\n",
      "grad_c:  tensor([-0.2368, -0.1082]) grad_a:  tensor([0.0007, 0.0153]) grad_p: tensor([-4.4588e-06, -3.3054e-06])\n",
      "grad_c:  tensor([-1.8662, -0.2781]) grad_a:  tensor([-0.0032,  0.0030]) grad_p: tensor([-0.0002, -0.0001])\n",
      "grad_c:  tensor([-1.2283, -2.4199]) grad_a:  tensor([-0.0119,  0.0058]) grad_p: tensor([-0.0004, -0.0003])\n",
      "grad_c:  tensor([-2.2863, -2.2053]) grad_a:  tensor([-0.0032,  0.0023]) grad_p: tensor([-0.0002, -0.0001])\n",
      "grad_c:  tensor([  5.0118, -13.0213]) grad_a:  tensor([0.0010, 0.0005]) grad_p: tensor([-1.8760e-05, -1.3534e-05])\n",
      "grad_c:  tensor([1.0817, 4.8481]) grad_a:  tensor([-0.0002, -0.0002]) grad_p: tensor([-9.4940e-07, -6.8557e-07])\n",
      "grad_c:  tensor([-19.3400,  -3.6255]) grad_a:  tensor([-0.0056, -0.0015]) grad_p: tensor([1.1421e-04, 8.4619e-05])\n",
      "grad_c:  tensor([-0.2084, -0.0073]) grad_a:  tensor([-0.0037, -0.0007]) grad_p: tensor([0.7390, 0.5831])\n",
      "grad_c:  tensor([-3.6772, -2.3261]) grad_a:  tensor([ 0.0005, -0.0009]) grad_p: tensor([0.5618, 0.4317])\n",
      "grad_c:  tensor([-3.2892, -3.4504]) grad_a:  tensor([-0.0010,  0.0007]) grad_p: tensor([1.0776, 0.8242])\n",
      "grad_c:  tensor([-3.4692, -0.0602]) grad_a:  tensor([-0.0001,  0.0001]) grad_p: tensor([1.4353, 1.0839])\n",
      "grad_c:  tensor([-0.0326, 11.5164]) grad_a:  tensor([-3.8655e-05, -3.8773e-06]) grad_p: tensor([0.8860, 0.7102])\n",
      "grad_c:  tensor([0.0501, 0.3450]) grad_a:  tensor([-0.0333,  0.0932]) grad_p: tensor([-0.0475, -0.0351])\n",
      "grad_c:  tensor([-21.5078,  -5.1764]) grad_a:  tensor([-0.0082,  0.0592]) grad_p: tensor([-3.5178e-05, -3.0814e-05])\n",
      "grad_c:  tensor([-0.2170,  0.1747]) grad_a:  tensor([-0.0097,  0.0139]) grad_p: tensor([1.4698e-04, 6.1178e-05])\n",
      "grad_c:  tensor([-3.4545, -0.2066]) grad_a:  tensor([-0.0007,  0.0042]) grad_p: tensor([1.3655e-04, 6.2239e-05])\n",
      "grad_c:  tensor([-2.7115, -1.6985]) grad_a:  tensor([-4.8615e-05,  2.5317e-03]) grad_p: tensor([ 5.6603e-06, -3.0382e-07])\n",
      "grad_c:  tensor([-1.8895,  2.9407]) grad_a:  tensor([-0.0003, -0.0003]) grad_p: tensor([-1.0011e-04, -7.3853e-05])\n",
      "grad_c:  tensor([0.2976, 3.7254]) grad_a:  tensor([-1.1602e-04, -9.8284e-05]) grad_p: tensor([-2.5572, -2.0345])\n",
      "grad_c:  tensor([0.5924, 0.2193]) grad_a:  tensor([-0.0005,  0.0100]) grad_p: tensor([-0.0002, -0.0001])\n",
      "grad_c:  tensor([-2.9744,  1.5068]) grad_a:  tensor([-0.0025,  0.0036]) grad_p: tensor([-0.0004, -0.0003])\n",
      "grad_c:  tensor([-2.7251, -1.0608]) grad_a:  tensor([-0.0057, -0.0049]) grad_p: tensor([-0.0005, -0.0004])\n",
      "grad_c:  tensor([-3.1415, -1.1049]) grad_a:  tensor([-0.0025, -0.0003]) grad_p: tensor([-1.2738e-04, -9.2233e-05])\n",
      "grad_c:  tensor([ 3.4320, -9.3700]) grad_a:  tensor([-0.0004, -0.0008]) grad_p: tensor([-0.0010, -0.0008])\n",
      "grad_c:  tensor([0.5826, 4.3385]) grad_a:  tensor([-0.0003, -0.0005]) grad_p: tensor([1.4315e-05, 1.1315e-05])\n",
      "grad_c:  tensor([-9.8968,  8.1857]) grad_a:  tensor([-0.0107,  0.0002]) grad_p: tensor([0.0012, 0.0009])\n",
      "grad_c:  tensor([-0.0114,  0.0777]) grad_a:  tensor([0.0132, 0.0213]) grad_p: tensor([-1.0366e-04, -7.8263e-05])\n",
      "grad_c:  tensor([-6.3041, -0.4001]) grad_a:  tensor([ 0.0006, -0.0020]) grad_p: tensor([1.1977, 0.7530])\n",
      "grad_c:  tensor([ 3.9396, -8.5047]) grad_a:  tensor([-0.0031,  0.0006]) grad_p: tensor([2.3705, 1.4865])\n",
      "grad_c:  tensor([ 2.2205, -9.4714]) grad_a:  tensor([-0.0005,  0.0008]) grad_p: tensor([0.9561, 0.6103])\n",
      "grad_c:  tensor([ 2.8340, -8.0271]) grad_a:  tensor([ 0.0003, -0.0005]) grad_p: tensor([0.0003, 0.0002])\n",
      "grad_c:  tensor([1.3126, 5.6346]) grad_a:  tensor([2.2248e-05, 3.2455e-05]) grad_p: tensor([2.2040e-05, 1.7481e-05])\n",
      "grad_c:  tensor([3.4286, 1.7433]) grad_a:  tensor([0.0167, 0.0169]) grad_p: tensor([5.5449e-05, 4.4477e-05])\n",
      "grad_c:  tensor([-12.0949,   5.8906]) grad_a:  tensor([0.0028, 0.0014]) grad_p: tensor([1.1246, 0.4864])\n",
      "grad_c:  tensor([-4.9470,  3.1077]) grad_a:  tensor([-0.0048, -0.0115]) grad_p: tensor([0.7104, 0.4237])\n",
      "grad_c:  tensor([ 0.0099, -3.6984]) grad_a:  tensor([-0.0011, -0.0023]) grad_p: tensor([0.0497, 0.0307])\n",
      "grad_c:  tensor([ 2.1947, -8.3417]) grad_a:  tensor([-8.7691e-05,  6.6974e-05]) grad_p: tensor([0.0010, 0.0007])\n",
      "grad_c:  tensor([ 0.4172, -1.2193]) grad_a:  tensor([-1.6095e-04, -2.2095e-05]) grad_p: tensor([7.3224e-06, 5.0979e-06])\n",
      "grad_c:  tensor([-0.4410,  0.3272]) grad_a:  tensor([-0.0079,  0.0111]) grad_p: tensor([2.7851e-05, 2.2286e-05])\n",
      "grad_c:  tensor([-23.0946,   7.3997]) grad_a:  tensor([-0.0069,  0.0067]) grad_p: tensor([1.5349, 0.8590])\n",
      "grad_c:  tensor([-0.3460, -0.6335]) grad_a:  tensor([-0.0085, -0.0029]) grad_p: tensor([1.2409, 0.7308])\n",
      "grad_c:  tensor([ 0.2505, -6.8433]) grad_a:  tensor([-0.0064, -0.0008]) grad_p: tensor([-0.5504, -0.4736])\n",
      "grad_c:  tensor([  1.8413, -12.2341]) grad_a:  tensor([-0.0020,  0.0007]) grad_p: tensor([-0.5338, -0.4024])\n",
      "grad_c:  tensor([ 1.6630, -6.8732]) grad_a:  tensor([-0.0023,  0.0012]) grad_p: tensor([-0.0027, -0.0022])\n",
      "grad_c:  tensor([-3.7231, 11.8435]) grad_a:  tensor([-5.3800e-07, -6.0413e-07]) grad_p: tensor([0.0002, 0.0002])\n",
      "grad_c:  tensor([-24.5020,  -2.5665]) grad_a:  tensor([0.0109, 0.0011]) grad_p: tensor([-0.0092, -0.0073])\n",
      "grad_c:  tensor([-3.6450,  0.3711]) grad_a:  tensor([-0.0052, -0.0067]) grad_p: tensor([1.2764, 0.9156])\n",
      "grad_c:  tensor([-0.8382, -4.9613]) grad_a:  tensor([ 0.0055, -0.0017]) grad_p: tensor([0.6441, 0.4250])\n",
      "grad_c:  tensor([-0.8581, -2.5859]) grad_a:  tensor([-0.0003, -0.0005]) grad_p: tensor([0.5465, 0.3058])\n",
      "grad_c:  tensor([-1.7635, -1.9896]) grad_a:  tensor([-0.0002,  0.0002]) grad_p: tensor([0.1721, 0.0421])\n",
      "grad_c:  tensor([-1.5100,  4.7994]) grad_a:  tensor([ 1.3485e-05, -2.5719e-04]) grad_p: tensor([0.5915, 0.3701])\n",
      "grad_c:  tensor([-0.1854,  4.3379]) grad_a:  tensor([-0.0021, -0.0121]) grad_p: tensor([-0.0006, -0.0005])\n",
      "grad_c:  tensor([-9.3079,  2.7955]) grad_a:  tensor([-0.0033, -0.0171]) grad_p: tensor([0.8982, 0.3333])\n",
      "grad_c:  tensor([-0.2964, -3.8824]) grad_a:  tensor([ 0.0018, -0.0053]) grad_p: tensor([0.4105, 0.2025])\n",
      "grad_c:  tensor([-1.2659, -3.0363]) grad_a:  tensor([-0.0014, -0.0006]) grad_p: tensor([ 0.2939, -0.0384])\n",
      "grad_c:  tensor([ 1.7034, -7.2960]) grad_a:  tensor([ 0.0010, -0.0004]) grad_p: tensor([-0.4038, -0.5506])\n",
      "grad_c:  tensor([0.1745, 4.4691]) grad_a:  tensor([-5.4094e-05, -2.3263e-04]) grad_p: tensor([-0.0057, -0.0067])\n",
      "grad_c:  tensor([-0.5476,  4.3318]) grad_a:  tensor([0.0005, 0.0059]) grad_p: tensor([-0.0012, -0.0010])\n",
      "grad_c:  tensor([-1.3734,  6.2960]) grad_a:  tensor([-0.0010,  0.0023]) grad_p: tensor([0.9843, 0.4024])\n",
      "grad_c:  tensor([-1.6611, -2.2253]) grad_a:  tensor([-0.0123, -0.0003]) grad_p: tensor([0.5036, 0.1979])\n",
      "grad_c:  tensor([-0.0067, -6.4745]) grad_a:  tensor([-0.0029, -0.0021]) grad_p: tensor([0.5474, 0.3115])\n",
      "grad_c:  tensor([ 0.1135, -7.1292]) grad_a:  tensor([-0.0001, -0.0010]) grad_p: tensor([0.1053, 0.0668])\n",
      "grad_c:  tensor([-0.8586, -1.2438]) grad_a:  tensor([-0.0002,  0.0002]) grad_p: tensor([0.0079, 0.0029])\n",
      "grad_c:  tensor([0.1373, 8.7860]) grad_a:  tensor([-1.6995e-05, -8.7022e-06]) grad_p: tensor([1.9511e-04, 7.7140e-05])\n",
      "grad_c:  tensor([-1.3668, -0.2546]) grad_a:  tensor([0.0011, 0.0133]) grad_p: tensor([-0.0001, -0.0001])\n",
      "grad_c:  tensor([-2.0175,  8.4028]) grad_a:  tensor([0.0006, 0.0197]) grad_p: tensor([0.7083, 0.3945])\n",
      "grad_c:  tensor([-3.1036, -1.1113]) grad_a:  tensor([-0.0084, -0.0002]) grad_p: tensor([0.6739, 0.3108])\n",
      "grad_c:  tensor([-0.6558, -4.0053]) grad_a:  tensor([-0.0027, -0.0015]) grad_p: tensor([-0.1649, -0.1790])\n",
      "grad_c:  tensor([-0.8652, -3.1013]) grad_a:  tensor([ 0.0009, -0.0015]) grad_p: tensor([0.0768, 0.0444])\n",
      "grad_c:  tensor([ 0.8772, -6.5348]) grad_a:  tensor([ 0.0017, -0.0001]) grad_p: tensor([1.6861e-04, 5.9072e-05])\n",
      "grad_c:  tensor([0.0533, 2.2182]) grad_a:  tensor([-0.0003, -0.0002]) grad_p: tensor([3.9254e-05, 1.7678e-05])\n",
      "grad_c:  tensor([-0.2818, -0.1510]) grad_a:  tensor([0.0244, 0.0402]) grad_p: tensor([-3.5056e-05, -6.2541e-05])\n",
      "grad_c:  tensor([-10.4344,   2.4047]) grad_a:  tensor([-0.0019,  0.0129]) grad_p: tensor([0.5190, 0.2934])\n",
      "grad_c:  tensor([-1.5290, -1.9997]) grad_a:  tensor([0.0009, 0.0068]) grad_p: tensor([0.4265, 0.2007])\n",
      "grad_c:  tensor([-2.9529, -4.8291]) grad_a:  tensor([0.0002, 0.0025]) grad_p: tensor([0.3143, 0.1834])\n",
      "grad_c:  tensor([-1.2862, -9.8361]) grad_a:  tensor([0.0003, 0.0004]) grad_p: tensor([0.0004, 0.0002])\n",
      "grad_c:  tensor([-0.2244, -5.8023]) grad_a:  tensor([-0.0014,  0.0006]) grad_p: tensor([1.0238e-04, 4.4775e-05])\n",
      "grad_c:  tensor([ 0.2704, 18.5371]) grad_a:  tensor([6.7686e-06, 1.3510e-05]) grad_p: tensor([4.1990e-05, 1.8629e-05])\n",
      "grad_c:  tensor([-0.8210,  0.5216]) grad_a:  tensor([0.0181, 0.0341]) grad_p: tensor([-0.0001, -0.0001])\n",
      "grad_c:  tensor([-20.8330,  10.0258]) grad_a:  tensor([-0.0162,  0.0023]) grad_p: tensor([0.2874, 0.0532])\n",
      "grad_c:  tensor([-1.9911, -1.0372]) grad_a:  tensor([0.0032, 0.0015]) grad_p: tensor([0.4829, 0.1176])\n",
      "grad_c:  tensor([-4.1760, -3.1236]) grad_a:  tensor([-9.6739e-04,  9.2781e-06]) grad_p: tensor([1.9101, 1.0857])\n",
      "grad_c:  tensor([-3.7952, -7.5862]) grad_a:  tensor([-0.0003,  0.0001]) grad_p: tensor([1.2534, 0.7327])\n",
      "grad_c:  tensor([-2.4908, -6.8123]) grad_a:  tensor([-0.0006,  0.0005]) grad_p: tensor([0.0203, 0.0123])\n",
      "grad_c:  tensor([ 0.4475, 13.6809]) grad_a:  tensor([-6.9656e-05, -3.8188e-05]) grad_p: tensor([0.0004, 0.0003])\n",
      "grad_c:  tensor([-6.6296,  3.8032]) grad_a:  tensor([0.0342, 0.0610]) grad_p: tensor([-0.0002, -0.0002])\n",
      "grad_c:  tensor([-1.8275, -1.0978]) grad_a:  tensor([-0.0106,  0.0006]) grad_p: tensor([1.4542, 0.6675])\n",
      "grad_c:  tensor([-4.0388, -3.0037]) grad_a:  tensor([-0.0045, -0.0017]) grad_p: tensor([1.4184, 0.7843])\n",
      "grad_c:  tensor([-3.0119, -7.2205]) grad_a:  tensor([-0.0006, -0.0003]) grad_p: tensor([1.2776, 0.7557])\n",
      "grad_c:  tensor([-1.0724, -3.0617]) grad_a:  tensor([-2.9812e-05, -5.5569e-05]) grad_p: tensor([1.4717e-04, 9.0973e-05])\n",
      "grad_c:  tensor([ 0.7555, 17.9272]) grad_a:  tensor([-9.5078e-06, -6.3304e-06]) grad_p: tensor([-6.1244e-06, -5.0655e-06])\n",
      "grad_c:  tensor([-0.8388, -0.6006]) grad_a:  tensor([0.0876, 0.1162]) grad_p: tensor([-0.0001, -0.0001])\n",
      "grad_c:  tensor([-0.3725, -0.0952]) grad_a:  tensor([0.0173, 0.1099]) grad_p: tensor([1.1437, 0.5950])\n",
      "grad_c:  tensor([-4.0895,  7.4856]) grad_a:  tensor([-0.0311,  0.0297]) grad_p: tensor([0.6724, 0.1025])\n",
      "grad_c:  tensor([-1.5118, -2.2403]) grad_a:  tensor([-0.0075,  0.0108]) grad_p: tensor([0.5297, 0.1594])\n",
      "grad_c:  tensor([-4.1695, -2.6547]) grad_a:  tensor([-0.0075, -0.0020]) grad_p: tensor([0.7868, 0.4777])\n",
      "grad_c:  tensor([-2.5269, -7.1513]) grad_a:  tensor([-0.0023,  0.0002]) grad_p: tensor([0.0016, 0.0010])\n",
      "grad_c:  tensor([-8.3929,  2.5580]) grad_a:  tensor([-8.4998e-04,  9.3778e-05]) grad_p: tensor([6.7828e-06, 4.1211e-06])\n",
      "grad_c:  tensor([-2.1048, 12.6392]) grad_a:  tensor([-1.3257e-04, -1.3678e-05]) grad_p: tensor([-2.8626e-05, -2.2520e-05])\n",
      "grad_c:  tensor([0.0175, 1.6199]) grad_a:  tensor([-0.0115,  0.0254]) grad_p: tensor([-8.1763e-05, -6.4005e-05])\n",
      "grad_c:  tensor([-3.0019,  0.6438]) grad_a:  tensor([0.0019, 0.0228]) grad_p: tensor([0.4840, 0.3479])\n",
      "grad_c:  tensor([-3.0491, -1.1541]) grad_a:  tensor([-0.0086, -0.0077]) grad_p: tensor([0.4191, 0.1415])\n",
      "grad_c:  tensor([-3.9677, -3.4604]) grad_a:  tensor([-4.1898e-05,  3.5816e-04]) grad_p: tensor([0.8864, 0.4692])\n",
      "grad_c:  tensor([-2.9784, -4.8028]) grad_a:  tensor([-0.0003,  0.0007]) grad_p: tensor([0.7267, 0.4400])\n",
      "grad_c:  tensor([-0.9616, -2.8985]) grad_a:  tensor([-4.4290e-05, -1.1198e-03]) grad_p: tensor([4.1743e-05, 2.1684e-05])\n",
      "grad_c:  tensor([-4.0736, 10.9394]) grad_a:  tensor([-1.0343e-06, -1.2047e-06]) grad_p: tensor([1.7056e-06, 2.6354e-07])\n",
      "grad_c:  tensor([-16.1432,  -5.1280]) grad_a:  tensor([-2.1605e-03,  3.0085e-05]) grad_p: tensor([-1.7601e-05, -1.3712e-05])\n",
      "grad_c:  tensor([-3.6341, -0.9656]) grad_a:  tensor([-0.0016, -0.0002]) grad_p: tensor([0.6125, 0.4135])\n",
      "grad_c:  tensor([-2.3888, -2.2285]) grad_a:  tensor([0.0009, 0.0014]) grad_p: tensor([0.1812, 0.3059])\n",
      "grad_c:  tensor([-3.2411, -2.3632]) grad_a:  tensor([-0.0001,  0.0007]) grad_p: tensor([0.2805, 0.3025])\n",
      "grad_c:  tensor([-2.0829, -6.2151]) grad_a:  tensor([-0.0010,  0.0014]) grad_p: tensor([0.0363, 0.0378])\n",
      "grad_c:  tensor([-1.4690, -5.6178]) grad_a:  tensor([ 0.0011, -0.0003]) grad_p: tensor([-1.5637, -1.3568])\n",
      "grad_c:  tensor([-0.0002,  0.0003]) grad_a:  tensor([-4.1316e-06, -3.1549e-06]) grad_p: tensor([-0.4971, -0.4269])\n",
      "grad_c:  tensor([-0.0636,  0.0267]) grad_a:  tensor([-0.0135, -0.0181]) grad_p: tensor([-9.5722e-05, -7.4443e-05])\n",
      "grad_c:  tensor([-1.0826,  7.2504]) grad_a:  tensor([-0.0042, -0.0020]) grad_p: tensor([-0.0033, -0.0028])\n",
      "grad_c:  tensor([-2.5607, -0.4650]) grad_a:  tensor([ 0.0013, -0.0021]) grad_p: tensor([-0.8423, -0.6367])\n",
      "grad_c:  tensor([-2.3391, -3.1806]) grad_a:  tensor([ 0.0003, -0.0016]) grad_p: tensor([-1.4926, -1.1250])\n",
      "grad_c:  tensor([-3.2922, -1.3755]) grad_a:  tensor([-0.0002, -0.0016]) grad_p: tensor([-0.0771, -0.0589])\n",
      "grad_c:  tensor([-1.3049e-04,  9.3159e-05]) grad_a:  tensor([-1.9480e-04,  3.5317e-05]) grad_p: tensor([-0.0342, -0.0320])\n",
      "grad_c:  tensor([-1.3098,  1.1888]) grad_a:  tensor([-3.4220e-05, -1.2459e-04]) grad_p: tensor([-0.0262, -0.0249])\n",
      "grad_c:  tensor([1.1081, 7.8551]) grad_a:  tensor([0.0032, 0.0064]) grad_p: tensor([-1.4949e-05, -1.1603e-05])\n",
      "grad_c:  tensor([-11.1333,   5.1410]) grad_a:  tensor([-9.5475e-05,  5.6354e-03]) grad_p: tensor([-0.5408, -0.4086])\n",
      "grad_c:  tensor([-0.9145,  0.1147]) grad_a:  tensor([-0.0109,  0.0063]) grad_p: tensor([-0.2957, -0.2243])\n",
      "grad_c:  tensor([-0.0091,  0.0258]) grad_a:  tensor([-0.0003,  0.0004]) grad_p: tensor([-0.0999, -0.0757])\n",
      "grad_c:  tensor([-3.7824, -4.4855]) grad_a:  tensor([-0.0012, -0.0013]) grad_p: tensor([-0.0072, -0.0054])\n",
      "grad_c:  tensor([-3.0186, -2.9169]) grad_a:  tensor([-0.0002, -0.0007]) grad_p: tensor([-3.5778e-05, -2.6735e-05])\n",
      "grad_c:  tensor([-0.0007, -0.0003]) grad_a:  tensor([1.3969e-06, 1.1008e-05]) grad_p: tensor([-6.1022e-06, -4.7295e-06])\n",
      "grad_c:  tensor([-0.7792,  1.5495]) grad_a:  tensor([-0.0080,  0.0121]) grad_p: tensor([-4.6491e-05, -3.6567e-05])\n",
      "grad_c:  tensor([-4.1366,  0.1706]) grad_a:  tensor([-0.0441, -0.0455]) grad_p: tensor([-0.1262, -0.0955])\n",
      "grad_c:  tensor([-2.0977, -2.4504]) grad_a:  tensor([-0.0119, -0.0034]) grad_p: tensor([-0.8847, -0.6701])\n",
      "grad_c:  tensor([-3.1417, -2.0783]) grad_a:  tensor([-0.0098, -0.0070]) grad_p: tensor([-0.0236, -0.0178])\n",
      "grad_c:  tensor([-2.2823, -1.6660]) grad_a:  tensor([0.0003, 0.0036]) grad_p: tensor([-0.0017, -0.0013])\n",
      "grad_c:  tensor([-0.0036, -0.0030]) grad_a:  tensor([6.3247e-05, 2.3988e-04]) grad_p: tensor([-4.9224e-06, -3.6792e-06])\n",
      "grad_c:  tensor([-0.7328, -0.3256]) grad_a:  tensor([ 0.0020, -0.0026]) grad_p: tensor([-1.0764e-06, -8.3697e-07])\n",
      "grad_c:  tensor([-8.5454,  0.8382]) grad_a:  tensor([-0.0064,  0.0206]) grad_p: tensor([-0.0042, -0.0031])\n",
      "grad_c:  tensor([-1.7236,  0.1848]) grad_a:  tensor([-0.0066,  0.0144]) grad_p: tensor([-0.0656, -0.0496])\n",
      "grad_c:  tensor([-11.3599,   2.9487]) grad_a:  tensor([-0.0037,  0.0053]) grad_p: tensor([-0.0110, -0.0083])\n",
      "grad_c:  tensor([-1.9329, -9.8101]) grad_a:  tensor([-4.0042e-04,  1.6480e-06]) grad_p: tensor([-0.0019, -0.0015])\n",
      "grad_c:  tensor([-1.0230, -2.6201]) grad_a:  tensor([ 0.0013, -0.0007]) grad_p: tensor([-1.8035e-05, -1.3505e-05])\n",
      "grad_c:  tensor([ 0.5781, 11.4277]) grad_a:  tensor([3.9857e-06, 4.6673e-06]) grad_p: tensor([-5.8362e-07, -4.5226e-07])\n",
      "grad_c:  tensor([-5.2163, -0.2553]) grad_a:  tensor([0.0016, 0.0082]) grad_p: tensor([-4.9797e-06, -3.9033e-06])\n",
      "grad_c:  tensor([-5.1065,  0.0899]) grad_a:  tensor([-0.0096,  0.0118]) grad_p: tensor([-0.0062, -0.0047])\n",
      "grad_c:  tensor([-1.3630, -3.6338]) grad_a:  tensor([-0.0120,  0.0076]) grad_p: tensor([-0.0459, -0.0343])\n",
      "grad_c:  tensor([-0.9803, -3.4661]) grad_a:  tensor([-0.0066, -0.0004]) grad_p: tensor([-0.0017, -0.0012])\n",
      "grad_c:  tensor([-1.1708, -2.9638]) grad_a:  tensor([-0.0027,  0.0015]) grad_p: tensor([-6.4266e-05, -4.8210e-05])\n",
      "grad_c:  tensor([-0.0378, -0.1049]) grad_a:  tensor([-2.8218e-04,  6.9596e-05]) grad_p: tensor([0., 0.])\n",
      "grad_c:  tensor([-1.9825,  6.1863]) grad_a:  tensor([-1.3402e-04, -1.9690e-06]) grad_p: tensor([-1.0866e-06, -8.4507e-07])\n",
      "grad_c:  tensor([-17.9539,  -7.0051]) grad_a:  tensor([ 0.0029, -0.0026]) grad_p: tensor([-9.8021e-05, -7.7753e-05])\n",
      "grad_c:  tensor([-1.4442, -1.9484]) grad_a:  tensor([-0.0073,  0.0141]) grad_p: tensor([-0.0021, -0.0016])\n",
      "grad_c:  tensor([-1.6785, -6.1497]) grad_a:  tensor([-0.0175,  0.0297]) grad_p: tensor([-0.7409, -0.5593])\n",
      "grad_c:  tensor([-1.9661, -4.3152]) grad_a:  tensor([-0.0056,  0.0040]) grad_p: tensor([-1.9702, -1.4814])\n",
      "grad_c:  tensor([-1.2430, -3.7227]) grad_a:  tensor([-0.0009,  0.0011]) grad_p: tensor([-0.1651, -0.1230])\n",
      "grad_c:  tensor([-0.0032, -0.0068]) grad_a:  tensor([ 8.8845e-05, -2.4377e-04]) grad_p: tensor([-0.0110, -0.0081])\n",
      "grad_c:  tensor([-3.8982, -4.9854]) grad_a:  tensor([0.0154, 0.0037]) grad_p: tensor([-0.0028, -0.0021])\n",
      "grad_c:  tensor([-8.5714,  5.5135]) grad_a:  tensor([-0.0218,  0.0252]) grad_p: tensor([-0.0048, -0.0036])\n",
      "grad_c:  tensor([-0.8110, -3.5096]) grad_a:  tensor([-0.0021,  0.0034]) grad_p: tensor([-0.3035, -0.2280])\n",
      "grad_c:  tensor([-1.9324, -4.8885]) grad_a:  tensor([-0.0023, -0.0038]) grad_p: tensor([-1.4454, -1.0798])\n",
      "grad_c:  tensor([-1.5188, -8.3875]) grad_a:  tensor([-0.0008, -0.0002]) grad_p: tensor([-1.1876, -0.8775])\n",
      "grad_c:  tensor([-0.9192, -8.7034]) grad_a:  tensor([0.0002, 0.0002]) grad_p: tensor([-0.6107, -0.4483])\n",
      "grad_c:  tensor([-3.8923, 15.1590]) grad_a:  tensor([-2.4033e-04,  2.1578e-05]) grad_p: tensor([-4.9575, -4.1767])\n",
      "grad_c:  tensor([-17.0113,   7.1888]) grad_a:  tensor([-0.0190, -0.0144]) grad_p: tensor([-0.0001, -0.0001])\n",
      "grad_c:  tensor([-2.0651, -2.2187]) grad_a:  tensor([ 0.0017, -0.0109]) grad_p: tensor([-0.2397, -0.2085])\n",
      "grad_c:  tensor([  0.8247, -11.2812]) grad_a:  tensor([ 0.0059, -0.0150]) grad_p: tensor([-0.7571, -0.6613])\n",
      "grad_c:  tensor([-0.6748, -2.5062]) grad_a:  tensor([-0.0029, -0.0016]) grad_p: tensor([-0.6953, -0.6071])\n",
      "grad_c:  tensor([-0.0096, -0.0722]) grad_a:  tensor([ 0.0037, -0.0051]) grad_p: tensor([-0.2548, -0.2218])\n",
      "grad_c:  tensor([-1.5663e-05,  3.1382e-05]) grad_a:  tensor([-0.0015,  0.0012]) grad_p: tensor([-0.1205, -0.1048])\n",
      "grad_c:  tensor([-3.7476, 10.2037]) grad_a:  tensor([-0.0082,  0.0035]) grad_p: tensor([-3.5639e-06, -2.8006e-06])\n",
      "grad_c:  tensor([-6.5124,  0.9976]) grad_a:  tensor([-0.0143,  0.0142]) grad_p: tensor([-0.0218, -0.0192])\n",
      "grad_c:  tensor([-1.2200, -4.5574]) grad_a:  tensor([0.0009, 0.0025]) grad_p: tensor([-0.0042, -0.0036])\n",
      "grad_c:  tensor([-4.1574,  2.7737]) grad_a:  tensor([-0.0014, -0.0019]) grad_p: tensor([-0.0003, -0.0002])\n",
      "grad_c:  tensor([-3.4820, -2.8676]) grad_a:  tensor([ 0.0002, -0.0011]) grad_p: tensor([-7.6884e-05, -6.6542e-05])\n",
      "grad_c:  tensor([-0.0113,  0.0030]) grad_a:  tensor([-4.4734e-05, -3.0216e-04]) grad_p: tensor([-5.4966e-07, -4.2854e-07])\n",
      "grad_c:  tensor([ -2.6900, -11.3808]) grad_a:  tensor([ 0.0057, -0.0019]) grad_p: tensor([-3.0546e-06, -2.3992e-06])\n",
      "grad_c:  tensor([-13.2141,   2.8177]) grad_a:  tensor([-0.0026, -0.0015]) grad_p: tensor([-0.0007, -0.0006])\n",
      "grad_c:  tensor([-1.0768,  0.2856]) grad_a:  tensor([ 0.0056, -0.0011]) grad_p: tensor([-0.0028, -0.0024])\n",
      "grad_c:  tensor([-3.4413, -4.3669]) grad_a:  tensor([-0.0035,  0.0017]) grad_p: tensor([-0.0004, -0.0004])\n",
      "grad_c:  tensor([  5.2364, -16.5874]) grad_a:  tensor([-0.0011,  0.0015]) grad_p: tensor([-0.0010, -0.0008])\n",
      "grad_c:  tensor([-0.5888, -1.3161]) grad_a:  tensor([-0.0003,  0.0006]) grad_p: tensor([-2.7446e-05, -2.3713e-05])\n",
      "grad_c:  tensor([-0.0006,  0.0018]) grad_a:  tensor([ 0.0007, -0.0013]) grad_p: tensor([-1.6551e-06, -1.2915e-06])\n",
      "grad_c:  tensor([-2.4794,  4.8165]) grad_a:  tensor([-0.0041, -0.0058]) grad_p: tensor([-1.0093e-05, -8.1374e-06])\n",
      "grad_c:  tensor([-2.8915,  2.1692]) grad_a:  tensor([-0.0051,  0.0001]) grad_p: tensor([-0.0005, -0.0004])\n",
      "grad_c:  tensor([-1.6221, -3.8495]) grad_a:  tensor([ 0.0021, -0.0010]) grad_p: tensor([-0.0034, -0.0029])\n",
      "grad_c:  tensor([ 0.3800, -1.3835]) grad_a:  tensor([-0.0033, -0.0009]) grad_p: tensor([-0.0003, -0.0003])\n",
      "grad_c:  tensor([-0.5691,  0.1693]) grad_a:  tensor([-0.0014,  0.0022]) grad_p: tensor([-0.0001, -0.0001])\n",
      "grad_c:  tensor([-5.2083e-05,  4.6940e-04]) grad_a:  tensor([-0.0002, -0.0011]) grad_p: tensor([-5.4807e-07, -4.2713e-07])\n",
      "grad_c:  tensor([-4.6209, 10.5224]) grad_a:  tensor([0.0004, 0.0030]) grad_p: tensor([-3.5891e-06, -2.8123e-06])\n",
      "grad_c:  tensor([-5.6305,  5.6313]) grad_a:  tensor([-0.0233, -0.0139]) grad_p: tensor([-0.0004, -0.0003])\n",
      "grad_c:  tensor([-1.5719, -2.4389]) grad_a:  tensor([ 4.7746e-04, -2.6879e-06]) grad_p: tensor([-0.0032, -0.0028])\n",
      "grad_c:  tensor([-2.4134, -4.7651]) grad_a:  tensor([-3.1552e-03, -8.6955e-05]) grad_p: tensor([-0.0005, -0.0004])\n",
      "grad_c:  tensor([-6.6585,  4.6300]) grad_a:  tensor([-0.0005,  0.0004]) grad_p: tensor([-0.0002, -0.0002])\n",
      "grad_c:  tensor([-19.2954,  13.4956]) grad_a:  tensor([-0.0001, -0.0001]) grad_p: tensor([-5.5000e-07, -4.2828e-07])\n",
      "grad_c:  tensor([10.0240, 14.0374]) grad_a:  tensor([ 0.0004, -0.0019]) grad_p: tensor([-1.5572e-06, -1.2207e-06])\n",
      "grad_c:  tensor([-7.6021,  5.7257]) grad_a:  tensor([-0.0061,  0.0033]) grad_p: tensor([-0.0004, -0.0004])\n",
      "grad_c:  tensor([-1.6032, -1.3873]) grad_a:  tensor([0.0075, 0.0059]) grad_p: tensor([-1.5232, -1.3092])\n",
      "grad_c:  tensor([-3.3967, -5.3963]) grad_a:  tensor([-0.0008, -0.0005]) grad_p: tensor([-2.3389, -1.9909])\n",
      "grad_c:  tensor([-12.1537,   3.0297]) grad_a:  tensor([-6.0682e-05, -1.4253e-04]) grad_p: tensor([-3.0234, -2.5093])\n",
      "grad_c:  tensor([-15.1726,   3.1691]) grad_a:  tensor([-3.3754e-05,  1.7577e-05]) grad_p: tensor([-0.0218, -0.0178])\n",
      "grad_c:  tensor([-0.0005,  0.0016]) grad_a:  tensor([-8.1153e-06, -2.9523e-06]) grad_p: tensor([-0.0027, -0.0022])\n",
      "grad_c:  tensor([14.0833, 16.3845]) grad_a:  tensor([-0.0026,  0.0132]) grad_p: tensor([-0.0030, -0.0024])\n",
      "grad_c:  tensor([-9.2893,  1.0441]) grad_a:  tensor([-0.0289,  0.0251]) grad_p: tensor([-0.2258, -0.1412])\n",
      "grad_c:  tensor([-1.7038, -2.3080]) grad_a:  tensor([-0.0148,  0.0228]) grad_p: tensor([-0.0039, -0.0872])\n",
      "grad_c:  tensor([-3.4470, -5.0170]) grad_a:  tensor([-0.0029,  0.0044]) grad_p: tensor([-0.2420, -0.1735])\n",
      "grad_c:  tensor([ 3.6330, -7.4428]) grad_a:  tensor([-0.0025,  0.0039]) grad_p: tensor([-0.1482, -0.1209])\n",
      "grad_c:  tensor([-7.7100e-05, -4.7563e-05]) grad_a:  tensor([-0.0006,  0.0010]) grad_p: tensor([-0.0129, -0.0105])\n",
      "grad_c:  tensor([-3.9224, -8.9550]) grad_a:  tensor([ 0.0229, -0.0033]) grad_p: tensor([-0.0007, -0.0005])\n",
      "grad_c:  tensor([-10.7155,   5.6312]) grad_a:  tensor([-0.0104,  0.0309]) grad_p: tensor([0.4220, 0.4268])\n",
      "grad_c:  tensor([-4.4268, -1.2506]) grad_a:  tensor([-0.0018, -0.0011]) grad_p: tensor([0.4224, 0.3174])\n",
      "grad_c:  tensor([-1.3237, -5.4321]) grad_a:  tensor([-0.0033, -0.0026]) grad_p: tensor([0.6699, 0.4104])\n",
      "grad_c:  tensor([ 2.8737, -5.1340]) grad_a:  tensor([2.6557e-04, 2.7095e-06]) grad_p: tensor([-0.0075, -0.0061])\n",
      "grad_c:  tensor([ 2.3967, -4.1206]) grad_a:  tensor([-1.2131e-03, -1.5810e-05]) grad_p: tensor([-6.6160e-05, -5.3510e-05])\n",
      "grad_c:  tensor([ 3.8579, 23.8058]) grad_a:  tensor([-5.9058e-05, -1.2005e-04]) grad_p: tensor([-2.0677e-05, -1.6277e-05])\n",
      "grad_c:  tensor([-16.2356,  20.4290]) grad_a:  tensor([-0.0066,  0.0047]) grad_p: tensor([-0.0002, -0.0001])\n",
      "grad_c:  tensor([-3.1402, -1.0104]) grad_a:  tensor([-0.0131, -0.0019]) grad_p: tensor([1.0001, 0.1957])\n",
      "grad_c:  tensor([-3.3709, -5.6347]) grad_a:  tensor([-7.8422e-03, -2.7733e-05]) grad_p: tensor([-0.1069, -0.0872])\n",
      "grad_c:  tensor([-3.0170, -8.8037]) grad_a:  tensor([-0.0011,  0.0002]) grad_p: tensor([-0.0062, -0.0050])\n",
      "grad_c:  tensor([-1.7329, -0.5613]) grad_a:  tensor([ 0.0004, -0.0002]) grad_p: tensor([-5.7646e-05, -4.6436e-05])\n",
      "grad_c:  tensor([1.4863, 9.4184]) grad_a:  tensor([ 0.0007, -0.0005]) grad_p: tensor([-5.9719e-06, -4.6317e-06])\n",
      "grad_c:  tensor([  3.7602, -15.1571]) grad_a:  tensor([0.0163, 0.0173]) grad_p: tensor([-5.7559e-05, -4.4820e-05])\n",
      "grad_c:  tensor([-5.2086, 17.2448]) grad_a:  tensor([-0.0007,  0.0073]) grad_p: tensor([1.6096, 0.7782])\n",
      "grad_c:  tensor([-3.4213,  0.6502]) grad_a:  tensor([-0.0208,  0.0141]) grad_p: tensor([1.5401, 0.6470])\n",
      "grad_c:  tensor([-2.9455, -3.3425]) grad_a:  tensor([-0.0098,  0.0133]) grad_p: tensor([0.8486, 0.4135])\n",
      "grad_c:  tensor([-1.1153, -2.7264]) grad_a:  tensor([-0.0020,  0.0034]) grad_p: tensor([0.4939, 0.3118])\n",
      "grad_c:  tensor([-0.5063, -1.0721]) grad_a:  tensor([4.9909e-05, 5.4176e-04]) grad_p: tensor([0.4196, 0.2768])\n",
      "grad_c:  tensor([-6.6856e-06,  7.8865e-05]) grad_a:  tensor([0.0006, 0.0005]) grad_p: tensor([2.0457e-06, 1.1245e-06])\n",
      "grad_c:  tensor([-8.0627, -7.4082]) grad_a:  tensor([0.0258, 0.0178]) grad_p: tensor([-3.1408e-05, -2.4438e-05])\n",
      "grad_c:  tensor([-10.1912,  -0.3814]) grad_a:  tensor([-0.0210,  0.0535]) grad_p: tensor([-0.0016, -0.0013])\n",
      "grad_c:  tensor([-2.1947, -2.1198]) grad_a:  tensor([0.0043, 0.0044]) grad_p: tensor([-0.0561, -0.0455])\n",
      "grad_c:  tensor([-3.3442, -5.3633]) grad_a:  tensor([-0.0045,  0.0004]) grad_p: tensor([-1.5367, -1.2530])\n",
      "grad_c:  tensor([ -4.1602, -10.8622]) grad_a:  tensor([-0.0019,  0.0047]) grad_p: tensor([-0.6378, -0.5102])\n",
      "grad_c:  tensor([ 24.4293, -58.8334]) grad_a:  tensor([-0.0002,  0.0006]) grad_p: tensor([-0.0012, -0.0009])\n",
      "grad_c:  tensor([-0.2263,  0.6361]) grad_a:  tensor([-1.5975e-05, -1.1649e-04]) grad_p: tensor([-0.0016, -0.0012])\n",
      "grad_c:  tensor([4.8775, 7.1291]) grad_a:  tensor([-0.0156, -0.0788]) grad_p: tensor([-0.0057, -0.0044])\n",
      "grad_c:  tensor([-13.0254,   3.7835]) grad_a:  tensor([-0.0427, -0.0070]) grad_p: tensor([-0.0114, -0.0089])\n",
      "grad_c:  tensor([0.0720, 0.1617]) grad_a:  tensor([-0.0045, -0.0069]) grad_p: tensor([-0.6938, -0.5474])\n",
      "grad_c:  tensor([-3.1904, -5.7382]) grad_a:  tensor([ 0.0025, -0.0022]) grad_p: tensor([-0.6887, -0.5449])\n",
      "grad_c:  tensor([-3.3452, -7.9539]) grad_a:  tensor([0.0013, 0.0024]) grad_p: tensor([-0.6797, -0.5308])\n",
      "grad_c:  tensor([ 1.0310, -5.7605]) grad_a:  tensor([ 8.2687e-05, -7.9344e-05]) grad_p: tensor([-0.0293, -0.0227])\n",
      "grad_c:  tensor([2.8287, 5.9162]) grad_a:  tensor([ 0.0155, -0.0062]) grad_p: tensor([-0.0019, -0.0015])\n",
      "grad_c:  tensor([-12.3601,   7.8832]) grad_a:  tensor([-0.0104,  0.0105]) grad_p: tensor([-0.0247, -0.0193])\n",
      "grad_c:  tensor([-4.4208,  0.7663]) grad_a:  tensor([-0.0185, -0.0003]) grad_p: tensor([-0.7550, -0.5905])\n",
      "grad_c:  tensor([-3.3869, -5.3053]) grad_a:  tensor([-0.0073, -0.0027]) grad_p: tensor([-0.6535, -0.5108])\n",
      "grad_c:  tensor([-5.1942, -9.0253]) grad_a:  tensor([-1.4608e-04,  6.8415e-05]) grad_p: tensor([-0.0192, -0.0149])\n",
      "grad_c:  tensor([-2.7157, -5.2940]) grad_a:  tensor([1.3246e-04, 1.5628e-06]) grad_p: tensor([-0.0008, -0.0006])\n",
      "grad_c:  tensor([-6.9486, 26.5029]) grad_a:  tensor([ 3.5974e-05, -1.2832e-05]) grad_p: tensor([-1.2656e-04, -9.7682e-05])\n",
      "grad_c:  tensor([-19.3964,  -2.1728]) grad_a:  tensor([0.0239, 0.0074]) grad_p: tensor([-0.0004, -0.0003])\n",
      "grad_c:  tensor([-4.3856,  0.8980]) grad_a:  tensor([-0.0191, -0.0042]) grad_p: tensor([-0.0055, -0.0042])\n",
      "grad_c:  tensor([ 1.1410, -8.6032]) grad_a:  tensor([-0.0055,  0.0060]) grad_p: tensor([-0.8084, -0.6253])\n",
      "grad_c:  tensor([-1.7229, -2.5669]) grad_a:  tensor([-0.0016, -0.0048]) grad_p: tensor([-0.0480, -0.0369])\n",
      "grad_c:  tensor([-1.7816, -2.5796]) grad_a:  tensor([ 0.0010, -0.0012]) grad_p: tensor([-0.0075, -0.0058])\n",
      "grad_c:  tensor([-0.0066,  0.0921]) grad_a:  tensor([0.0002, 0.0003]) grad_p: tensor([-1.1381e-04, -8.6634e-05])\n",
      "grad_c:  tensor([11.6432,  7.3355]) grad_a:  tensor([ 0.0024, -0.0022]) grad_p: tensor([-4.4908e-05, -3.4430e-05])\n",
      "grad_c:  tensor([-19.7464,  13.5270]) grad_a:  tensor([-0.0158,  0.0066]) grad_p: tensor([-0.0126, -0.0096])\n",
      "grad_c:  tensor([-0.6792,  0.0900]) grad_a:  tensor([-0.0093,  0.0088]) grad_p: tensor([-0.5434, -0.4171])\n",
      "grad_c:  tensor([-3.6329, -5.0879]) grad_a:  tensor([-0.0039, -0.0032]) grad_p: tensor([-0.1342, -0.1026])\n",
      "grad_c:  tensor([-4.6248, -5.0240]) grad_a:  tensor([-0.0023, -0.0003]) grad_p: tensor([-0.0161, -0.0123])\n",
      "grad_c:  tensor([-1.1081, -1.2121]) grad_a:  tensor([-0.0003,  0.0006]) grad_p: tensor([-0.0003, -0.0002])\n",
      "grad_c:  tensor([-3.3740, 12.6524]) grad_a:  tensor([4.2715e-06, 7.1126e-05]) grad_p: tensor([-1.8868e-05, -1.4542e-05])\n",
      "grad_c:  tensor([ 4.7052, 19.1657]) grad_a:  tensor([0.0414, 0.0185]) grad_p: tensor([-1.7001e-06, -1.3022e-06])\n",
      "grad_c:  tensor([-7.8610,  1.6526]) grad_a:  tensor([-0.0077, -0.0003]) grad_p: tensor([-0.1040, -0.0792])\n",
      "grad_c:  tensor([-0.1710, -0.3675]) grad_a:  tensor([-0.0044,  0.0031]) grad_p: tensor([-0.3095, -0.2352])\n",
      "grad_c:  tensor([-4.1613, -3.6879]) grad_a:  tensor([-0.0024, -0.0002]) grad_p: tensor([-0.0246, -0.0187])\n",
      "grad_c:  tensor([ 0.8522, -1.9743]) grad_a:  tensor([0.0002, 0.0010]) grad_p: tensor([-0.0030, -0.0022])\n",
      "grad_c:  tensor([ 2.6303, -4.6383]) grad_a:  tensor([-8.5620e-05, -3.1473e-04]) grad_p: tensor([-3.0309e-05, -2.2663e-05])\n",
      "grad_c:  tensor([-9.7929, -2.3811]) grad_a:  tensor([0.0508, 0.0449]) grad_p: tensor([-6.7786, -6.8064])\n",
      "grad_c:  tensor([-5.7083, -1.0076]) grad_a:  tensor([-0.0065,  0.0209]) grad_p: tensor([-0.0060, -0.0046])\n",
      "grad_c:  tensor([-1.8175, -2.5255]) grad_a:  tensor([-0.0076,  0.0030]) grad_p: tensor([-0.1264, -0.0985])\n",
      "grad_c:  tensor([-4.0614, -3.6870]) grad_a:  tensor([-0.0032, -0.0034]) grad_p: tensor([-0.0251, -0.0202])\n",
      "grad_c:  tensor([-4.1713, -5.4947]) grad_a:  tensor([-0.0002, -0.0002]) grad_p: tensor([-0.0079, -0.0067])\n",
      "grad_c:  tensor([-2.8761, -4.7439]) grad_a:  tensor([-4.3413e-05, -2.7378e-05]) grad_p: tensor([-0.0012, -0.0011])\n",
      "grad_c:  tensor([-17.4218,  27.0252]) grad_a:  tensor([0.0088, 0.0156]) grad_p: tensor([36.9878, 34.3764])\n",
      "grad_c:  tensor([-1.5187,  0.0659]) grad_a:  tensor([-0.0144, -0.0030]) grad_p: tensor([-0.0024, -0.0019])\n",
      "grad_c:  tensor([  1.9106, -14.1388]) grad_a:  tensor([-7.1804e-03,  1.1130e-06]) grad_p: tensor([-0.0002, -0.0001])\n",
      "grad_c:  tensor([-6.2078, -5.4199]) grad_a:  tensor([-0.0029, -0.0005]) grad_p: tensor([0.0017, 0.0016])\n",
      "grad_c:  tensor([-1.2863, -1.1019]) grad_a:  tensor([0.0005, 0.0008]) grad_p: tensor([0.0507, 0.0462])\n",
      "grad_c:  tensor([-5.3297, 16.7917]) grad_a:  tensor([-5.1099e-06, -3.7885e-06]) grad_p: tensor([0.0128, 0.0117])\n",
      "grad_c:  tensor([  1.8948, -20.5957]) grad_a:  tensor([0.0297, 0.0649]) grad_p: tensor([0.4830, 0.4361])\n",
      "grad_c:  tensor([-17.2674,   2.5094]) grad_a:  tensor([0.0149, 0.0542]) grad_p: tensor([-1.8634e-05, -1.4280e-05])\n",
      "grad_c:  tensor([-3.0208, -0.2113]) grad_a:  tensor([-0.0009,  0.0128]) grad_p: tensor([-0.0013, -0.0010])\n",
      "grad_c:  tensor([  3.2616, -15.0967]) grad_a:  tensor([ 0.0008, -0.0030]) grad_p: tensor([-1.0687e-04, -8.2760e-05])\n",
      "grad_c:  tensor([ 0.3121, -1.0656]) grad_a:  tensor([-0.0072,  0.0020]) grad_p: tensor([0.0002, 0.0001])\n",
      "grad_c:  tensor([-0.0007,  0.0010]) grad_a:  tensor([-0.0007, -0.0009]) grad_p: tensor([0.0340, 0.0299])\n",
      "grad_c:  tensor([-0.0002, -0.0003]) grad_a:  tensor([-1.7583e-04,  2.3650e-05]) grad_p: tensor([0.0093, 0.0081])\n",
      "grad_c:  tensor([ 0.5847, -3.8679]) grad_a:  tensor([0.0431, 0.0605]) grad_p: tensor([0.0401, 0.0348])\n",
      "grad_c:  tensor([-15.1314,   4.1817]) grad_a:  tensor([0.0179, 0.0347]) grad_p: tensor([-1.1744e-05, -8.9793e-06])\n",
      "grad_c:  tensor([-6.7916,  4.5359]) grad_a:  tensor([0.0003, 0.0061]) grad_p: tensor([-0.0008, -0.0006])\n",
      "grad_c:  tensor([-0.0075,  0.0190]) grad_a:  tensor([-0.0047,  0.0020]) grad_p: tensor([0.0005, 0.0004])\n",
      "grad_c:  tensor([-0.0017,  0.0017]) grad_a:  tensor([-0.0010,  0.0007]) grad_p: tensor([0.1561, 0.1317])\n",
      "grad_c:  tensor([-1.3255, -0.3237]) grad_a:  tensor([-2.0731e-07, -3.5192e-07]) grad_p: tensor([0.0010, 0.0009])\n",
      "grad_c:  tensor([ 0.1738, -0.4773]) grad_a:  tensor([0.0070, 0.0300]) grad_p: tensor([2.7052e-05, 2.2500e-05])\n",
      "grad_c:  tensor([-14.1641,  10.6601]) grad_a:  tensor([0.0467, 0.0044]) grad_p: tensor([-5.4862e-05, -4.1665e-05])\n",
      "grad_c:  tensor([-0.5709,  1.3301]) grad_a:  tensor([0.0042, 0.0081]) grad_p: tensor([-0.0016, -0.0012])\n",
      "grad_c:  tensor([ 0.5154, -1.1642]) grad_a:  tensor([7.9565e-06, 1.2743e-05]) grad_p: tensor([0.7207, 0.5890])\n",
      "grad_c:  tensor([-2.1400, -0.6465]) grad_a:  tensor([0., 0.]) grad_p: tensor([0.0386, 0.0317])\n",
      "grad_c:  tensor([-0.3410,  0.7001]) grad_a:  tensor([-0.0206, -0.0050]) grad_p: tensor([0., 0.])\n",
      "grad_c:  tensor([-11.1253,   4.0876]) grad_a:  tensor([-0.0390,  0.0035]) grad_p: tensor([-0.0087, -0.0069])\n",
      "grad_c:  tensor([ 0.2455, -2.4152]) grad_a:  tensor([0.0014, 0.0047]) grad_p: tensor([-1.0916, -0.8604])\n",
      "grad_c:  tensor([ 0.5468, -2.7021]) grad_a:  tensor([0.0002, 0.0002]) grad_p: tensor([-1.1394, -0.8798])\n",
      "grad_c:  tensor([  9.9039, -22.2523]) grad_a:  tensor([2.2416e-06, 2.8912e-06]) grad_p: tensor([-0.3874, -0.2922])\n",
      "grad_c:  tensor([ 0.0004, -0.0010]) grad_a:  tensor([0., 0.]) grad_p: tensor([-0.0021, -0.0015])\n",
      "grad_c:  tensor([-16.5355,  17.4225]) grad_a:  tensor([-0.0053, -0.0054]) grad_p: tensor([-0.0150, -0.0109])\n",
      "grad_c:  tensor([-0.9584,  0.5147]) grad_a:  tensor([-0.0146, -0.0046]) grad_p: tensor([-7.7735e-05, -5.7832e-05])\n",
      "grad_c:  tensor([ 0.4026, -1.7017]) grad_a:  tensor([-1.3566e-04,  1.7330e-05]) grad_p: tensor([-0.1371, -0.1021])\n",
      "grad_c:  tensor([-1.0199, -0.5882]) grad_a:  tensor([ 0.0005, -0.0005]) grad_p: tensor([-0.0934, -0.0694])\n",
      "grad_c:  tensor([-1.0360, -0.7280]) grad_a:  tensor([ 0.0004, -0.0004]) grad_p: tensor([0.0398, 0.0278])\n",
      "grad_c:  tensor([-10.1382,  30.0648]) grad_a:  tensor([9.2519e-08, 2.6257e-07]) grad_p: tensor([0.1096, 0.0788])\n",
      "grad_c:  tensor([-0.0471,  0.2599]) grad_a:  tensor([-0.0248, -0.0526]) grad_p: tensor([0.0284, 0.0206])\n",
      "grad_c:  tensor([-20.3792,  10.1637]) grad_a:  tensor([-0.0452, -0.0057]) grad_p: tensor([-1.8572e-05, -1.3665e-05])\n",
      "grad_c:  tensor([-1.2232,  0.7613]) grad_a:  tensor([-0.0012,  0.0074]) grad_p: tensor([-0.0081, -0.0060])\n",
      "grad_c:  tensor([-2.3256, -5.4810]) grad_a:  tensor([0.0020, 0.0037]) grad_p: tensor([-0.0218, -0.0161])\n",
      "grad_c:  tensor([ 0.1296, -0.4710]) grad_a:  tensor([0.0004, 0.0008]) grad_p: tensor([-0.1028, -0.0760])\n",
      "grad_c:  tensor([-0.0005,  0.0015]) grad_a:  tensor([0.0001, 0.0002]) grad_p: tensor([-0.0609, -0.0443])\n",
      "grad_c:  tensor([ 0.0076, -0.0108]) grad_a:  tensor([-4.1295e-07, -2.0247e-07]) grad_p: tensor([-0.0143, -0.0102])\n",
      "grad_c:  tensor([-0.5950,  0.8930]) grad_a:  tensor([-0.0702, -0.0245]) grad_p: tensor([-0.0166, -0.0119])\n",
      "grad_c:  tensor([-2.6070,  3.6116]) grad_a:  tensor([-0.0050, -0.0094]) grad_p: tensor([-0.0061, -0.0045])\n",
      "grad_c:  tensor([-18.6242,  16.3317]) grad_a:  tensor([ 0.0024, -0.0024]) grad_p: tensor([-1.0906, -0.8085])\n",
      "grad_c:  tensor([  5.6365, -15.5432]) grad_a:  tensor([-0.0038,  0.0040]) grad_p: tensor([-0.1345, -0.0989])\n",
      "grad_c:  tensor([ 8.1625e-05, -1.3151e-04]) grad_a:  tensor([-7.2601e-06,  5.0390e-06]) grad_p: tensor([-0.0102, -0.0074])\n",
      "grad_c:  tensor([ 17.0628, -32.3304]) grad_a:  tensor([ 0.0239, -0.0821]) grad_p: tensor([4.2732, 3.6654])\n",
      "grad_c:  tensor([5.5439, 8.4718]) grad_a:  tensor([-0.0020,  0.0774]) grad_p: tensor([-2.2773e-05, -1.7194e-05])\n",
      "grad_c:  tensor([-5.8885,  0.8659]) grad_a:  tensor([-0.0201, -0.0016]) grad_p: tensor([-0.0046, -0.0035])\n",
      "grad_c:  tensor([-6.5336,  0.9503]) grad_a:  tensor([0.0003, 0.0003]) grad_p: tensor([-0.3539, -0.2718])\n",
      "grad_c:  tensor([-4.1951, -0.8242]) grad_a:  tensor([-0.0019,  0.0018]) grad_p: tensor([-1.3975, -1.0655])\n",
      "grad_c:  tensor([ 0.0013, -0.0040]) grad_a:  tensor([-0.0003,  0.0002]) grad_p: tensor([-1.9671, -1.4700])\n",
      "grad_c:  tensor([ 0.0107, -0.0159]) grad_a:  tensor([-3.9465e-05,  2.4405e-05]) grad_p: tensor([-0.0072, -0.0052])\n",
      "grad_c:  tensor([-0.0335,  0.1983]) grad_a:  tensor([0.0222, 0.0081]) grad_p: tensor([0.6439, 0.4957])\n",
      "grad_c:  tensor([-5.2408,  6.9870]) grad_a:  tensor([-0.0229, -0.0026]) grad_p: tensor([-0.0025, -0.0019])\n",
      "grad_c:  tensor([-10.7672,  10.5193]) grad_a:  tensor([-0.0027, -0.0015]) grad_p: tensor([-1.4175, -1.0595])\n",
      "grad_c:  tensor([ 16.0761, -43.6860]) grad_a:  tensor([-0.0006, -0.0007]) grad_p: tensor([-0.2206, -0.1629])\n",
      "grad_c:  tensor([ 2.6885e-06, -1.0125e-06]) grad_a:  tensor([-9.0149e-05, -1.1180e-04]) grad_p: tensor([-0.4484, -0.3249])\n",
      "grad_c:  tensor([-4.6205, 12.3131]) grad_a:  tensor([-0.0268,  0.0440]) grad_p: tensor([0.0084, 0.0087])\n",
      "grad_c:  tensor([-0.0802,  0.1982]) grad_a:  tensor([-0.0174,  0.0115]) grad_p: tensor([-0.0011, -0.0008])\n",
      "grad_c:  tensor([ 10.9049, -38.6471]) grad_a:  tensor([-0.0042, -0.0018]) grad_p: tensor([-1.9742, -1.4047])\n",
      "grad_c:  tensor([  8.8016, -22.2518]) grad_a:  tensor([-0.0025, -0.0038]) grad_p: tensor([-1.2934, -0.9080])\n",
      "grad_c:  tensor([ 0.5817, -1.0540]) grad_a:  tensor([-0.0007, -0.0010]) grad_p: tensor([-1.2971, -0.8668])\n",
      "grad_c:  tensor([-10.8893,  31.0303]) grad_a:  tensor([-4.8682e-05, -5.7875e-05]) grad_p: tensor([-0.0086, -0.0048])\n",
      "grad_c:  tensor([  6.0758, -16.5838]) grad_a:  tensor([-0.0026,  0.0042]) grad_p: tensor([-0.0277, -0.0164])\n",
      "grad_c:  tensor([-8.9631, 19.7618]) grad_a:  tensor([-0.0025,  0.0066]) grad_p: tensor([-7.6859e-05, -5.3298e-05])\n",
      "grad_c:  tensor([-0.4654,  0.6547]) grad_a:  tensor([-0.0055, -0.0054]) grad_p: tensor([-0.0006, -0.0004])\n",
      "grad_c:  tensor([-1.3876, -2.2852]) grad_a:  tensor([-0.0005,  0.0002]) grad_p: tensor([-0.8294, -0.5785])\n",
      "grad_c:  tensor([ 11.4055, -27.9508]) grad_a:  tensor([-0.0032,  0.0013]) grad_p: tensor([-0.8566, -0.5691])\n",
      "grad_c:  tensor([-0.0109, -0.0059]) grad_a:  tensor([-0.0003, -0.0009]) grad_p: tensor([-0.3160, -0.1935])\n",
      "grad_c:  tensor([ 0.0032, -0.0048]) grad_a:  tensor([-0.0009,  0.0007]) grad_p: tensor([0.0342, 0.0255])\n",
      "grad_c:  tensor([ 1.3540, -4.5116]) grad_a:  tensor([-0.0432,  0.2094]) grad_p: tensor([0.0006, 0.0004])\n",
      "grad_c:  tensor([-7.8369, 12.3196]) grad_a:  tensor([-0.0097,  0.0059]) grad_p: tensor([-0.0019, -0.0014])\n",
      "grad_c:  tensor([-0.6962, -0.0914]) grad_a:  tensor([0.0047, 0.0014]) grad_p: tensor([-0.2777, -0.1966])\n",
      "grad_c:  tensor([  3.9462, -11.5797]) grad_a:  tensor([0.0026, 0.0028]) grad_p: tensor([-0.3430, -0.2378])\n",
      "grad_c:  tensor([  9.9161, -24.0025]) grad_a:  tensor([-0.0026,  0.0052]) grad_p: tensor([0.2719, 0.1863])\n",
      "grad_c:  tensor([-3.0525, -2.1290]) grad_a:  tensor([-0.0005,  0.0008]) grad_p: tensor([0.2182, 0.1275])\n",
      "grad_c:  tensor([-0.0001, -0.0018]) grad_a:  tensor([-2.2430e-05,  3.3711e-05]) grad_p: tensor([0.0022, 0.0014])\n",
      "grad_c:  tensor([-5.9698, 11.7197]) grad_a:  tensor([-0.0282, -0.0229]) grad_p: tensor([0.0004, 0.0002])\n",
      "grad_c:  tensor([-0.5227,  0.7688]) grad_a:  tensor([-0.0335,  0.0155]) grad_p: tensor([0.4802, 0.3322])\n",
      "grad_c:  tensor([-4.7384, -4.2610]) grad_a:  tensor([-0.0179,  0.0171]) grad_p: tensor([0.9634, 0.6399])\n",
      "grad_c:  tensor([ 0.1855, -0.4622]) grad_a:  tensor([-0.0055,  0.0058]) grad_p: tensor([0.0566, 0.0385])\n",
      "grad_c:  tensor([ 0.0606, -0.0916]) grad_a:  tensor([-0.0066,  0.0079]) grad_p: tensor([0.0455, 0.0331])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aa = 0.01  # reduced actor learning rate\n",
    "alpha_c = 0.01  # reduced critic learning rate\n",
    "gama = 0.1\n",
    "lam = 0.8\n",
    "\n",
    "xa_sample, ya_sample = [], []\n",
    "xp_sample, yp_sample = [], []\n",
    "xc_sample, yc_sample = [], []\n",
    "elig_trace = dict()\n",
    "\n",
    "# xk = np.array([1, 1])\n",
    "xk = np.array([1,1])\n",
    "uk = np.array([random()])\n",
    "\n",
    "xp_sample.append(np.concatenate((xk, uk)))\n",
    "xa_sample.append(xk)\n",
    "xc_sample.append(xk)\n",
    "\n",
    "ya_sample.append(uk.item())\n",
    "yc_sample.append(0.0)\n",
    "log, log_a, log_c = [], [], []\n",
    "\n",
    "Wa = np.array([1,1,1,1])\n",
    "Wp = np.array([1, 1, 1, 1,1])\n",
    "\n",
    "converge_cnt = 0\n",
    "converge_cnta = 0\n",
    "converge_cntc = 0\n",
    "# B = B.reshape(4,1)\n",
    "\n",
    "func_a = model(2,1)\n",
    "opti_a = torch.optim.Adam(func_a.parameters(), lr=0.01)\n",
    "func_p = model(3,2)\n",
    "opti_p = torch.optim.Adam(func_p.parameters(), lr=0.01)\n",
    "func_c = model(2,1)\n",
    "opti_c = torch.optim.Adam(func_c.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(500):\n",
    "    noise = random()\n",
    "    xk = A @ xk + B @ uk\n",
    "    if np.linalg.norm(xk) > 15:  \n",
    "        xk = np.random.rand(2,)\n",
    "    xk = np.round(xk, decimals=2)\n",
    "\n",
    "    yp_sample.append(xk)\n",
    "    rk = -(xk @ xk + uk.item() ** 2)\n",
    "\n",
    "\n",
    "\n",
    "    # Actor update\n",
    "    # beta_a, mask_idxA = LLR(xa_sample, ya_sample, xk, 9, Wa)\n",
    "    alias_a = deepcopy(xk)\n",
    "    grad_a, mask_idxA = DNN(func_a,opti_a,9,xa_sample,ya_sample,alias_a)\n",
    "    alias_a = torch.tensor(alias_a).float()\n",
    "    uk = saturate((func_a(alias_a) + 0.001 * noise).detach().numpy())\n",
    "\n",
    "    # Policy model\n",
    "    # beta_p,_= LLR(xp_sample, yp_sample, xp_sample[-1], 9, Wp)\n",
    "    alias_p = deepcopy(xp_sample[-1])    \n",
    "    grad_p, _ = DNN(func_p,opti_p,9,xp_sample,yp_sample,alias_p,True)\n",
    "    xp_sample.append(np.concatenate((xk, uk)))\n",
    "\n",
    "    # Critic update \n",
    "    # beta_c, mask_idxC = LLR(xc_sample, yc_sample, xk, 20, Wa)\n",
    "    alias_c = deepcopy(xk)\n",
    "    grad_c, mask_idxC= DNN(func_c,opti_c,20,xc_sample,yc_sample,alias_c)\n",
    "\n",
    "\n",
    "    for m in mask_idxA:\n",
    "        try:\n",
    "            ya_sample[m] = saturate(ya_sample[m] + aa * grad_c.numpy() @ grad_p.numpy()) #use grad_p here\n",
    "        except Exception:\n",
    "            pass  \n",
    "\n",
    "    # Vk = beta_c @ np.concatenate((xk, [1]))\n",
    "    alias_c = torch.tensor(alias_c).float()\n",
    "    Vk = func_c(alias_c).detach().numpy()\n",
    "    delta_k = rk + gama * Vk - yc_sample[-1]\n",
    "    # print(Vk,delta_k)\n",
    "\n",
    "    # Update eligibility traces\n",
    "    for j in mask_idxC:\n",
    "        key = xc_sample[j].tobytes()\n",
    "        elig_trace[key] = 1.0\n",
    "        yc_sample[j] += alpha_c * delta_k * 1.0\n",
    "\n",
    "    all_idxs = set(range(len(yc_sample)))\n",
    "    other_idxs = all_idxs - set(mask_idxC)\n",
    "\n",
    "    for k in other_idxs:\n",
    "        key = xc_sample[i].tobytes()\n",
    "        elig_trace[key] = elig_trace.get(key, 0) * gama * lam\n",
    "        elig_trace[key] = min(elig_trace[key], 5.0) \n",
    "        yc_sample[k] += alpha_c * delta_k * elig_trace[key]\n",
    "    xa_sample.append(xk)\n",
    "    ya_sample.append(uk.item())\n",
    "    xc_sample.append(xk)\n",
    "    yc_sample.append(Vk)\n",
    "    print(\"grad_c: \",grad_c, \"grad_a: \",grad_a,\"grad_p:\", grad_p)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aa87fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<bound method Module.parameters of Sequential(\n",
       "   (0): Linear(in_features=2, out_features=10, bias=True)\n",
       "   (1): Tanh()\n",
       "   (2): Linear(in_features=10, out_features=1, bias=True)\n",
       " )>,\n",
       " Parameter containing:\n",
       " tensor([[-3.1467, -2.4381],\n",
       "         [-1.7937, -1.0890],\n",
       "         [ 1.9362,  1.7865],\n",
       "         [-0.4440, -0.4891],\n",
       "         [-0.1201,  1.2809],\n",
       "         [-0.2673,  0.3075],\n",
       "         [-1.5314,  1.8598],\n",
       "         [-0.7279, -0.6085],\n",
       "         [-0.2980,  0.2084],\n",
       "         [ 2.2343,  2.2345]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-5.3943, -3.7022,  1.9518, -0.7023,  2.2056,  0.3548, -1.2008, -0.6030,\n",
       "          0.6504,  2.1071], requires_grad=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_a.parameters,func_a[0].weight,func_a[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a6b2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<bound method Module.parameters of Sequential(\n",
       "   (0): Linear(in_features=2, out_features=10, bias=True)\n",
       "   (1): Tanh()\n",
       "   (2): Linear(in_features=10, out_features=1, bias=True)\n",
       " )>,\n",
       " Parameter containing:\n",
       " tensor([[ 0.2817, -0.7328],\n",
       "         [ 3.1496, -7.6767],\n",
       "         [ 1.0007,  0.8430],\n",
       "         [-1.1050,  2.3796],\n",
       "         [ 2.1602, -4.2822],\n",
       "         [ 2.7334, -4.1337],\n",
       "         [ 1.1583, -1.0015],\n",
       "         [-2.9462,  4.2893],\n",
       "         [ 0.2624,  0.9978],\n",
       "         [-1.1690,  2.0501]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 12.9791,   8.7935,  -4.5947,   5.0956,  -0.1716,   6.2491, -14.2754,\n",
       "         -10.0855,   4.5147, -11.8379], requires_grad=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_c.parameters,func_c[0].weight,func_c[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d63e576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<bound method Module.parameters of Sequential(\n",
       "   (0): Linear(in_features=3, out_features=10, bias=True)\n",
       "   (1): Tanh()\n",
       "   (2): Linear(in_features=10, out_features=2, bias=True)\n",
       " )>,\n",
       " Parameter containing:\n",
       " tensor([[ -2.2016,  -2.9492,  -4.6408],\n",
       "         [  1.5882,  -1.0697,  -1.4058],\n",
       "         [  2.0962,  -1.4885,   0.7006],\n",
       "         [ -0.4745,  -0.7656,  -5.4954],\n",
       "         [ -0.5014,   1.2152,   0.2516],\n",
       "         [  0.9020,  -0.2884,   3.2327],\n",
       "         [ -3.5127,  -5.9321,  -5.8208],\n",
       "         [ -4.2047,   3.4150,   9.5494],\n",
       "         [  0.5012,   1.1966,   4.3109],\n",
       "         [ -1.3262,  -0.5320, -10.0855]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ -6.0113,  10.2727,  -3.5397,  -6.3362, -15.0920,   6.9607,  -2.5715,\n",
       "          12.0230,   6.1377,  -3.1943], requires_grad=True))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_p.parameters,func_p[0].weight,func_p[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40322b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(func_a.state_dict(),'weights/actor_param_new.pt')\n",
    "torch.save(func_c.state_dict(),'weights/critic_params_new.pt')\n",
    "torch.save(func_p.state_dict(),'weights/process_params_new.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
